<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Bolin Lai</title>
    <meta name="author" content="Bolin Lai">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:60%;vertical-align:middle">
                <p class="name" style="text-align: center;">Bolin Lai</p>
                <p>Hi! I am a 5th-year PhD student in the Machine Learning Program of Georgia Institute of Technology, advised by Prof. <a href="https://rehg.org/">James Rehg</a> and co-advised by Prof. <a href="https://faculty.cc.gatech.edu/~zk15/">Zsolt Kira</a>. Currently, I'm also a visiting student at CS department of UIUC. Prior to my PhD, I got my Master's degree majoring in ECE and Bachelor's degree majoring in Information Engineering from Shanghai Jiao Tong University. I worked with Prof. <a href="https://scholar.google.com/citations?user=pbjw9sMAAAAJ&hl=en&oi=ao">Ya Zhang</a> during my master.</p>
                <p>I interned at Meta GenAI (now Meta Superintelligence Labs) for three times working on research of generative models. My projects span multimodal LLMs, image/video diffusion models, autoregressive architectures and fundamental research on tokenizers. Please check my employment experiences and publications below for details.</p>
                <p style="text-align:center">
                  <a href="mailto:bolin.lai@gatech.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=lWrljmQAAAAJ&hl=en"> Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/BolinLai">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/bolin-lai-625b78139">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/bryanislucky">Twitter</a>
                </p>
              </td>
              <td style="padding:2.5%;width:35%;max-width:40%">
                <img style="width:100%;max-width:80%;object-fit:cover;" alt="profile photo" src="figures/photo.png">
                <!-- <img style="width:100%;max-width:80%;object-fit:cover;border-radius: 50%;" alt="profile photo" src="figures/photo.png"> -->
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <p><font color="#FF8080">I'm looking for a full-time Research Scientist / Applied Scientist / ML Engineer position (available starting Dec. 2025). Please drop me an email if you think I'm a good fit in your team.</font></p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
              </td>
            </tr>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <p>
                  My research interests lie in <b>Multimodal Learning</b>, including <b>Multimodal Understanding</b> (e.g., VLMs, MLLMs) and <b>Image/Video Generation</b> (e.g., diffusion, flow matching). 
                  My career goal is to build omni multimodal systems that can understand, reason, and generate across text, image, video, and audio -- by integrating LLM planning/reasoning agents and high-fidelity diffusion backends into one autoregressive architecture.
                </p>
                <p><span style="color:#1a237e;">I'm always looking for self-motivated graduate/ungraduate students to collaborate with. Don't hesitate to reach out to me if you are interested in my research.</span></p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>News</h2>
              </td>
            </tr>
            <tr>
              <td style="padding:5px;width:100%;vertical-align:middle">
                <div style="font-size:14px;color:#888;margin-bottom:2px;">Scroll for more news ‚Üì</div>
                <div style="position:relative;">
                  <div id="news-scroll" style="max-height:320px; overflow-y:auto; padding-right:12px;">
                    <ul>
                      <li>Sep 2025: üéâ One paper got accepted by NeurIPS2025.</li>
                      <li>Aug 2025: üë®‚Äçüíª‚Äç I'm on the job market now! Seeking a position starting in Dec. 2025 or Jan. 2026.</li>
                      <li>Jun 2025: I was recognized as an <a href="https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee#all-outstanding-reviewer">outstanding reviewer</a> in CVPR.</li>
                      <li>Jun 2025: üèÖ Our LEGO paper was recognized as Distinguished Paper by <a href="https://egovis.github.io/awards/2023_2024/">EgoVis</a> at CVPR2025.</li>
                      <li>May 2025: I started my internship at Meta AGI Foundations in Seattle working with Dr. <a href="https://imisra.github.io/">Ishan Misra</a>.</li>
                      <li>Mar 2025: üéâ I successfully passed my thesis proposal. Thank all committee members (<a href="https://rehg.org/">James Rehg</a>, <a href="https://faculty.cc.gatech.edu/~zk15/">Zsolt Kira</a>, <a href="https://faculty.cc.gatech.edu/~hays/">James Hays</a>, <a href="https://faculty.cc.gatech.edu/~judy/">Judy Hoffman</a>)! I'll be on the job market in September of 2025. üë®‚Äçüíª‚Äç</li>
                      <li>Feb 2025: üéâ I have one first-author paper (<a href="https://bolinlai.github.io/projects/InstaManip/">InstaManip</a>) and two co-author papers (<a href="https://arxiv.org/pdf/2501.04336">VideoMindPalace</a> and <a href="https://arxiv.org/pdf/2504.02244">SocialGesture</a>) accepted by CVPR 2025. Thank all collaborators. See you @Nashville.</li>
                      <li>Oct 2024: üîç We released a thorough <a href="https://arxiv.org/pdf/2410.14045">survey</a> in action anticipation. Please check out if you are interested in this field.</li>
                      <li>Oct 2024: üèÖ Our <a href="https://bolinlai.github.io/Lego_EgoActGen/">LEGO</a> paper was nominated in the <a href="./figures/eccv_finalist.jpg">Best Paper Finalist</a> @ECCV2024. Congratulations to all co-authors!</li>
                      <li>Aug 2024: üé§ Our LEGO paper got Oral presentation.</li>
                      <li>July 2024: üéâ Two first-author papers were accepted by ECCV! Please check out our latest work: <a href="https://bolinlai.github.io/Lego_EgoActGen/">LEGO</a> (for action generation) and <a href="https://bolinlai.github.io/CSTS-EgoGazeAnticipation/">CSTS</a> (for gaze forecasting). Thank all the co-authors!</li>
                      <li>May 2024: üë®‚Äçüíª‚Äçüë®‚Äçüíª‚Äç I started my second intenrship at GenAI, Meta in Bay Area.</li>
                      <li>Mar 2024: üéâ One co-author paper was accepted by CVPR (Oral). See you in Seattle!</li>
                      <li>Jul 2023: üéâ Our expansion of prior work GLC was accepted by IJCV!</li>
                      <li>May 2023: üë®‚Äçüíª‚Äç I started my internship at GenAI Meta in Bay Area!</li>
                      <li>Apr 2023: üéâ I successfully passed the qualifying exam.</li>
                      <li>Mar 2023: üéâ One paper was accepted to the Findings of ACL2023. Please check out our new dataset for social understanding: <a href="https://aclanthology.org/2023.findings-acl.411.pdf">Werewolf Among Us</a>.</li>
                      <li>Nov 2022: üèÖ We won the Best Student Paper Prize on BMVC. Thanks to all co-authors!</li>
                      <li>Sep 2022: üéâ Our work <a href="https://bolinlai.github.io/GLC-EgoGazeEst/">GLC</a> was accepted by <a href="https://bmvc2022.org/">BMVC 2022</a>!</li>
                      <li>Jan 2022: I started working with <a href="https://rehg.org/">Prof. James Rehg</a> at Georgia Tech.</li>
                    </ul>
                  </div>
                  <div id="news-gradient-top" style="position:absolute;left:0;right:0;top:0;height:32px;pointer-events:none;z-index:2;background:linear-gradient(to bottom,rgba(255,255,255,1),rgba(255,255,255,0.85) 80%,rgba(255,255,255,0));display:none;"></div>
                  <div style="position:absolute;left:0;right:0;bottom:0;height:32px;pointer-events:none;z-index:2;background:linear-gradient(to bottom,rgba(255,255,255,0),rgba(255,255,255,0.85) 80%,rgba(255,255,255,1));"></div>
                </div>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Employment</h2>
              </td>
            </tr>
            <tr>
              <td style="padding:5px;width:100%;vertical-align:middle">
                <ul>
                  <li><b>[May 2025 ‚Äì Present] Research Scientist Intern</b> <br>
                    <font color='#A51014'><i>Meta Superintelligence Labs, Multimedia Core Video Generation Team</i></font><br>
                    <ul style="margin-left:0px; list-style-type:circle;">
                      <li><i>Analyzing and improving the diffusibility of high-dimensional latent space (submitted to CVPR 2026).</i></li>
                      <li><i>Engineering experience on large-scale MovieGen codebase and distributed training.</i></li>
                      <li><i>Mentor &amp; Manager: <a href="https://imisra.github.io/">Ishan Misra</a></i></li>
                      <li><i>Collaborators: <a href="https://people.eecs.berkeley.edu/~xdwang/">Xudong Wang</a>, <a href="https://rohitgirdhar.github.io/">Rohit Girdhar</a>, <a href="https://rssaketh.github.io/">Saketh Rambhatla</a></i></li>
                    </ul>
                  </li>
                  <br>
                  <li><b>[May 2024 ‚Äì Dec. 2024] Research Scientist Intern</b> <br>
                    <font color='#A51014'><i>Meta GenAI, Llama Applied Multimodal Team</i></font><br>
                    <ul style="margin-left:0px; list-style-type:circle;">
                      <li><i>Omni autoregressive architecture for unified multimodal understanding and generation (CVPR 2025, Highlight).</i></li>
                      <li><i>Proposed novel group self-attention mechanism to improve reasoning capabilities of LLMs on multimodal tasks.</i></li>
                      <li><i>Mentor: <a href="http://xiaotong.me/">Tong Xiao</a> &nbsp;&nbsp;&nbsp;&nbsp; Manager: <a href="https://n-zhang.github.io/">Ning Zhang</a></i></li>
                      <li><i>Collaborators:  <a href="https://xujuefei.com/">Felix Juefei-Xu</a>, <a href="https://aptx4869lm.github.io/">Miao Liu</a>, <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a>, <a href="https://hockeybro12.github.io/">Nikhil Mehta</a>, <a href="https://cs.stanford.edu/~cgzhu/">Chenguang Zhu</a></i></li>
                    </ul>
                  </li>
                  <br>
                  <li><b>[May 2023 ‚Äì Dec. 2023] Research Scientist Intern</b> <br>
                    <font color='#A51014'><i>Meta GenAI, Llama Image/Video Data Team</i></font><br>
                    <ul style="margin-left:0px; list-style-type:circle;">
                      <li><i>LLM-enhanced instructional image generation in the real-world environment (ECCV 2024, Oral, Best Paper Finalist).</i></li>
                      <li><i>Proposed to bridge feature spaces of LLMs and diffusion models by learnable layers for complex action generation.</i></li>
                      <li><i>Mentor: <a href="https://aptx4869lm.github.io/">Miao Liu</a> &nbsp;&nbsp;&nbsp;&nbsp; Manager: <a href="https://www.linkedin.com/in/guan-pang-38aaaa29/">Guan Pang</a></i></li></i></li>
                      <li><i>Collaborators: <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a>, <a href="https://www.lawrencechen.me/">Lawrence Chen</a></i></li>
                    </ul>
                  </li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>

          <div class="pub-tabs" id="pub-tabs" style="padding:10px 10px 0 10px;">
            <button type="button" class="pub-tab-btn active" id="tab-selected">Selected</button>
            <button type="button" class="pub-tab-btn" id="tab-full">Full</button>
          </div>
          <div style="padding:6px 10px 6px 10px;color:#666;font-size:16px;">(The Selected tab highlights publications that best represent my expertise)</div>

          <div id="publications-section">
            <table class="pub-table" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/freqwarm.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">Toward Diffusible High-Dimensional Latent Spaces: A Frequency Perspective</span>
                  <br><br>
                  <strong>Bolin Lai</strong>,
                  <a href="https://people.eecs.berkeley.edu/~xdwang/">Xudong Wang</a>,
                  <a href="https://rssaketh.github.io/">Saketh Rambhatla</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>,
                  <a href="https://faculty.cc.gatech.edu/~zk15/">Zsolt Kira</a>,
                  <a href="https://rohitgirdhar.github.io/">Rohit Girdhar</a>,
                  <a href="https://imisra.github.io/">Ishan Misra</a>
                  <br>
                  <font color='#A51014'><i>Under Review</i>, 2025</font>
                  <br>
                  [Paper (coming soon)]
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/transgesture.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">Toward Human Deictic Gesture Target Estimation</span>
                  <br><br>
                  <a href="https://www.irohxucao.com/">Xu Cao</a>,
                  Pranav Virupaksha,
                  <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                  <strong>Bolin Lai</strong>,
                  <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                  <a href="https://whatashot.github.io/">Jintai Chen</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>
                  <br>
                  <font color='#A51014'><i>NeurIPS</i>, 2025</font>
                  <br>
                  [<a href="https://openreview.net/pdf?id=hio3T2OwHB">Paper</a>]
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/flexti2v.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training</span>
                  <br><br>
                  <strong>Bolin Lai</strong>,
                  <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                  <a href="https://www.irohxucao.com/">Xu Cao</a>,
                  <a href="https://ryanxli.github.io/">Xiang Li</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>
                  <br>
                  <font color='#A51014'><i>Under Review</i>, 2025</font>
                  <br>
                  <a href="https://bolinlai.github.io/projects/FlexTI2V/">Webpage</a> /
                  <a href="http://arxiv.org/pdf/2505.20629">Paper</a> /
                  <a href="https://github.com/BolinLai/FlexTI2V">Code</a>
                </td>
              </tr>

              <tr class="pub-item">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/vcr.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">Learning Predictive Visuomotor Coordination</span>
                  <br><br>
                  <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                  <strong>Bolin Lai</strong>,
                  <a href="https://aptx4869lm.github.io/">Miao Liu</a>,
                  <a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>
                  <br>
                  <font color='#A51014'><i>Under Review</i>, 2025</font>
                  <br>
                  <a href="https://vjwq.github.io/VCR/">Webpage</a> /
                  <a href="https://arxiv.org/pdf/2503.23300?">Paper</a>
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/online-mmsi.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">Towards Online Multi-Modal Social Interaction Understanding</span>
                  <br><br>
                  <a href="https://sampson-lee.github.io/">Xinpeng Li</a>,
                  Shijian Deng,
                  <strong>Bolin Lai</strong>,
                  <a href="https://scholar.google.com/citations?user=K-ObTwoAAAAJ&hl=en&oi=sra">Weiguo Pian</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>,
                  <a href="https://www.yapengtian.com/">Yapeng Tian</a>
                  <br>
                  <font color='#A51014'><i>Under Review</i>, 2025</font>
                  <br>
                  <a href="https://arxiv.org/pdf/2503.19851">Paper</a> /
                  <a href="https://github.com/Sampson-Lee/OnlineMMSI">Code</a>
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/instamanip.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation</span>
                  <br><br>
                  <strong>Bolin Lai</strong>,
                  <a href="https://xujuefei.com/">Felix Juefei-Xu</a>,
                  <a href="https://aptx4869lm.github.io/">Miao Liu</a>,
                  <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a>,
                  <a href="https://hockeybro12.github.io/">Nikhil Mehta</a>,
                  <a href="https://cs.stanford.edu/~cgzhu/">Chenguang Zhu</a>,
                  <a href="https://oodbag.github.io/">Zeyi Huang</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>,
                  <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                  <a href="https://n-zhang.github.io/">Ning Zhang</a>,
                  <a href="http://xiaotong.me/">Tong Xiao</a>
                  <br>
                  <font color='#A51014'><i>CVPR</i>, 2025 (Highlight)</font>
                  <br>
                  <a href="https://bolinlai.github.io/projects/InstaManip/">Webpage</a> /
                  <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation_CVPR_2025_paper.pdf">Paper</a> /
                  <a href="https://github.com/BolinLai/InstaManip">Code</a> /
                  <a href="https://huggingface.co/bolinlai/InstaManip">HuggingFace</a> /
                  <a href="https://openaccess.thecvf.com/content/CVPR2025/supplemental/Lai_Unleashing_In-context_Learning_CVPR_2025_supplemental.zip">Supplementary</a>
                  <a href="https://www.youtube.com/watch?v=JedJi_f-oQk">Video</a> /
                  <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32621.png?t=1748313084.6108608">Poster</a>
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/videomindpalace.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs</span>
                  <br><br>
                  <a href="https://oodbag.github.io/">Zeyi Huang</a>,
                  Yuyang Ji,
                  <a href="https://minione.github.io/">Xiaofang Wang</a>,
                  <a href="https://hockeybro12.github.io/">Nikhil Mehta</a>,
                  <a href="http://xiaotong.me/">Tong Xiao</a>,
                  Donghyun Lee,
                  <a href="https://www.linkedin.com/in/siggyv/">Sigmund VanValkenburgh</a>,
                  <a href="https://scholar.google.com/citations?user=QRvXHNsAAAAJ&hl=en">Shengxin Zha</a>,
                  <strong>Bolin Lai</strong>,
                  <a href="https://lichengunc.github.io/">Licheng Yu</a>,
                  <a href="https://n-zhang.github.io/">Ning Zhang</a>,
                  <a href="https://pages.cs.wisc.edu/~yongjaelee/"> Yong Jae Lee</a>,
                  <a href="https://aptx4869lm.github.io/">Miao Liu</a>
                  <br>
                  <font color='#A51014'><i>CVPR</i>, 2025</font>
                  <br>
                  [<a href="https://arxiv.org/pdf/2501.04336">Paper</a>]
                </td>
              </tr>
              <tr class="pub-item">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/socialgesture.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">SocialGesture: Delving into Multi-person Gesture Understanding</span>
                  <br><br>
                  <a href="https://www.irohxucao.com/">Xu Cao</a>,
                  Pranav Virupaksha,
                  <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                  <strong>Bolin Lai</strong>,
                  <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                  <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>
                  <br>
                  <font color='#A51014'><i>CVPR</i>, 2025</font>
                  <br>
                  <a href="https://www.irohxucao.com/SocialGesture/">Webpage</a> /
                  <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Cao_SocialGesture_Delving_into_Multi-person_Gesture_Understanding_CVPR_2025_paper.pdf">Paper</a> /
                  <a href="https://huggingface.co/datasets/IrohXu/SocialGesture">Dataset</a>
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/action_forecasting_survey.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">Human Action Anticipation: A Survey</span>
                  <br><br>
                  <strong>Bolin Lai</strong>*,
                  <a href="http://www.qxcv.net/research/">Sam Toyer*</a>,
                  <a href="https://tushar-n.github.io/">Tushar Nagarajan</a>,
                  <a href="http://rohitgirdhar.github.io/">Rohit Girdhar</a>,
                  <a href="https://scholar.google.com/citations?user=QRvXHNsAAAAJ&hl=en&oi=ao">Shengxin Zha</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>,
                  <a href="http://www.cs.cmu.edu/~kkitani">Kris Kitani</a>,
                  <a href="http://www.cs.utexas.edu/~grauman/">Kristen Grauman</a>,
                  <a href="https://rutadesai.github.io/">Ruta Desai</a>,
                  <a href="https://aptx4869lm.github.io/">Miao Liu</a>
                  <br>
                  <font color='#A51014'><i>Under Review of IJCV</i>, 2024</font>
                  <br>
                  [<a href="https://arxiv.org/pdf/2410.14045">Paper</a>]
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/vcog-bench.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">What is the Visual Cognition Gap between Humans and Multimodal LLMs?</span>
                  <br><br>
                  <a href="https://www.irohxucao.com/">Xu Cao</a>, 
                  <strong>Bolin Lai</strong>,
                  <a href="https://wenqian-ye.github.io/">Wenqian Ye</a>,
                  <a href="https://ysma.me/">Yunsheng Ma</a>,
                  <a href="https://scholar.google.com/citations?user=mNxy4CkAAAAJ&hl=en&oi=sra">Joerg Heintz</a>,
                  <a href="https://whatashot.github.io/">Jintai Chen</a>,
                  <a href="https://scholar.google.com/citations?user=dPPOpHUAAAAJ&hl=en&oi=sra">Jianguo Cao</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>
                  <br>
                  <font color='#A51014'><i>COLM</i>, 2025</font>
                  <br>
                  [<a href="https://arxiv.org/pdf/2406.10424">Paper</a>]
                </td>
              </tr>
              <tr class="pub-item">
                <td style="padding:2%;width:36%;vertical-align:middle;">
                  <img src='figures/mmspubench.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle;">
                  <span class="papertitle">MM-SPUBENCH: Towards Better Understanding of Spurious Biases in Multimodal LLMs</span>
                  <br><br>
                  <a href="https://wenqian-ye.github.io/">Wenqian Ye</a>,
                  <a href="https://gtzheng.github.io/">Guangtao Zheng</a>,
                  <a href="https://ysma.me/">Yunsheng Ma</a>,
                  <a href="https://www.irohxucao.com/">Xu Cao</a>, 
                  <strong>Bolin Lai</strong>,
                  <a href="https://rehg.org/">James M. Rehg</a>,
                  <a href="https://www.cs.virginia.edu/~az9eg/website/home.html">Aidong Zhang</a>
                  <br>
                  <font color='#A51014'><i>Under Review</i>, 2024</font>
                  <br>
                  [<a href="https://arxiv.org/pdf/2406.17126?">Paper</a>]
                </td>
              </tr>
              <tr class="pub-item">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/social_ai_survey.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">Towards Social AI: A Survey on Understanding Social Interactions</span>
                  <br><br>
                  <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                  <a href="https://yocodeyo.github.io/">Minzhi Li</a>,
                  <strong>Bolin Lai</strong>,
                  <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                  <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                  <a href="https://www.irohxucao.com/">Xu Cao</a>, 
                  <a href="http://karaozgur.com/">Ozgur Kara</a>,
                  <a href="https://scholar.google.com/citations?user=e4jgsUcAAAAJ&hl=en">Bikram Boote</a>,
                  <a href="http://wyshi.github.io/">Weiyan Shi</a>,
                  <a href="http://www.diyiyang.com/">Diyi Yang</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>
                  <br>
                  <font color='#A51014'><i>Under Review of TPAMI</i>, 2024</font>
                  <br>
                  [<a href="https://arxiv.org/pdf/2409.15316">Paper</a>]
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/lego.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">LEGO: <u>L</u>earning <u>EGO</u>centric Action Frame Generation via Visual Instruction Tuning</span>
                  <br><br>
                  <strong>Bolin Lai</strong>,
                  <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a>,
                  <a href="https://www.lawrencechen.me/">Lawrence Chen</a>,
                  <a href="https://www.linkedin.com/in/guan-pang-38aaaa29/">Guan Pang</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>,
                  <a href="https://aptx4869lm.github.io/">Miao Liu</a>
                  <br>
                  <font color='#A51014'><i>ECCV</i>, 2024 (Oral, Best Paper Finalist)</font><br>
                  <font color="#A51014">EgoVis Distinguished Paper Award</font>
                  <br>
                  <a href="https://bolinlai.github.io/Lego_EgoActGen/">Webpage</a> /
                  <a href="https://arxiv.org/pdf/2312.03849.pdf">Paper</a> /
                  <a href="https://github.com/BolinLai/LEGO">Code</a> /
                  <a href="https://huggingface.co/datasets/bolinlai/LEGO-Dataset">Dataset</a> /
                  <a href="https://huggingface.co/collections/bolinlai/lego-67b386cf642909c56776f754">HuggingFace</a> /
                  <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01383-supp.pdf">Supplementary</a> /
                  <a href="https://www.youtube.com/watch?v=90bNgwe4URU">Video</a> /
                  <a href="https://eccv2024.ecva.net/media/PosterPDFs/ECCV%202024/1257.png?t=1726963913.2047732">Poster</a> /
                  <a href="https://www.cc.gatech.edu/news/new-generative-tool-provides-images-accompany-step-step-instructions">Press: GT News</a>
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/avgaze.png' width="90%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation</span>
                  <br><br>
                  <strong>Bolin Lai</strong>,
                  <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                  <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                  <a href="https://aptx4869lm.github.io/">Miao Liu*</a>,
                  <a href="https://rehg.org/">James M. Rehg*</a>
                  <br>
                  <font color='#A51014'><i>ECCV</i>, 2024</font>
                  <br>
                  <a href="https://bolinlai.github.io/CSTS-EgoGazeAnticipation/">Webpage</a> /
                  <a href="https://arxiv.org/pdf/2305.03907.pdf">Paper</a> /
                  <a href="https://github.com/BolinLai/CSTS">Code</a> / 
                  <a href="https://github.com/BolinLai/CSTS/tree/main/data">Data Split</a> /
                  <a href="https://huggingface.co/bolinlai/CSTS">HuggingFace</a> /
                  <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01396-supp.pdf">Supplementary</a> /
                  <a href="https://www.youtube.com/watch?v=5ApMaLhlxUc">Video</a> /
                  <a href="https://eccv2024.ecva.net/media/PosterPDFs/ECCV%202024/268.png?t=1726967693.823367">Poster</a>
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/multimodal_social.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations</span>
                  <br><br>
                  <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                  <strong>Bolin Lai</strong>,
                  <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                  <a href="https://scholar.google.com/citations?user=e4jgsUcAAAAJ&hl=en">Bikram Boote</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>
                  <br>
                  <font color='#A51014'><i>CVPR</i>, 2024 (Oral)</font>
                  <br>
                  <a href="https://sangmin-git.github.io/projects/MMSI/">Webpage</a> /
                  <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Modeling_Multimodal_Social_Interactions_New_Challenges_and_Baselines_with_Densely_CVPR_2024_paper.pdf">Paper</a> /
                  <a href="https://github.com/sangmin-git/MMSI">Code</a> /
                  <a href="https://www.dropbox.com/scl/fo/fbv6njzu1ynbgv9wgtrwo/ANPk2TKqK2rl44MqKu05ogk?rlkey=yx7bmzmmiymauvz99q2rvjajg&st=305631zj&dl=0">Split & Annotations</a> /
                  <a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lee_Modeling_Multimodal_Social_CVPR_2024_supplemental.pdf">Supplementary</a>
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/deduction.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games</span>
                  <br><br>
                  <strong>Bolin Lai</strong>*,
                  <a href="https://icefoxzhx.github.io/">Hongxin Zhang*</a>,
                  <a href="https://aptx4869lm.github.io/">Miao Liu*</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=EnC_6s0AAAAJ">Aryan Pariani*</a>,
                  <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                  <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                  <a href="https://www.shirley.id/">Shirley Anugrah Hayati</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>,
                  <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
                  <br>
                  <font color='#A51014'><i>ACL Findings</i>, 2023</font>
                  <br>
                  <a href="https://bolinlai.github.io/projects/Werewolf-Among-Us/">Webpage</a> /
                  <a href="https://aclanthology.org/2023.findings-acl.411.pdf">Paper</a> /
                  <a href="https://github.com/SALT-NLP/PersuationGames">Code</a> /
                  <a href="https://huggingface.co/datasets/bolinlai/Werewolf-Among-Us">Dataset</a> /
                  <a href="https://www.youtube.com/watch?v=Uch5sUwDs2w">Video</a>
                  <!-- <p>Modeling social behaviors of multi-person scenarios using both video and language.</p> -->
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/gaze_est.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation and Beyond</span>
                  <br><br>
                  <strong>Bolin Lai</strong>,
                  <a href="https://aptx4869lm.github.io/">Miao Liu</a>,
                  <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>
                  <br>
                  <font color='#A51014'><i>International Journal of Computer Vision (IJCV)</i>, 2023</font>
                  <br>
                  <a href="https://bolinlai.github.io/GLC-EgoGazeEst/">Webpage</a> /
                  <a href="https://link.springer.com/article/10.1007/s11263-023-01879-7">Paper</a> /
                  <a href="https://github.com/BolinLai/GLC">Code</a>
                </td>
              </tr>
              <tr class="pub-item selected">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/glc.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation</span>
                  <br><br>
                  <strong>Bolin Lai</strong>,
                  <a href="https://aptx4869lm.github.io/">Miao Liu</a>,
                  <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                  <a href="https://rehg.org/">James M. Rehg</a>
                  <br>
                  <font color='#A51014'><i>BMVC</i>, 2022 (Spotlight, Best Student Paper)</font>
                  <br>
                  <a href="https://bolinlai.github.io/GLC-EgoGazeEst/">Webpage</a> /
                  <a href="https://bmvc2022.mpi-inf.mpg.de/0227.pdf">Paper</a> /
                  <a href="https://github.com/BolinLai/GLC">Code</a> /
                  <a href="https://bolinlai.github.io/GLC-EgoGazeEst/Ego4D_Gaze_Split.zip">Data Split</a> /
                  <a href="https://bmvc2022.mpi-inf.mpg.de/0227_supp.zip">Supplementary</a> /
                  <a href="https://bmvc2022.mpi-inf.mpg.de/0227_video.mp4">Video</a> /
                  <a href="https://bmvc2022.mpi-inf.mpg.de/0227_poster.pdf">Poster</a>
                  <!-- <p>Improving egocentric gaze estimation by explicitly modeling global-local correlations in transformer-based architecture.</p> -->
                </td>
              </tr>
            </tbody></table>

            <table class="pub-separator" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:15px;width:100%;vertical-align:middle;text-align:center;">
                  <div style="display:flex;align-items:center;justify-content:center;gap:5px;">
                    <div style="flex:1;height:2px;background:repeating-linear-gradient(to right, #ccc 0, #ccc 8px, transparent 8px, transparent 16px);"></div>
                    <span style="color:#666;font-size:16px;font-weight:500;white-space:nowrap;padding:0 20px;">Research before my PhD, mainly about medical image analysis</span>
                    <div style="flex:1;height:2px;background:repeating-linear-gradient(to right, #ccc 0, #ccc 8px, transparent 8px, transparent 16px);"></div>
                  </div>
                </td>
              </tr>
            </tbody></table>

            <table class="pub-table" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr class="pub-item">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/venibot.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">Semi-supervised Vein Segmentation of Ultrasound Images for Autonomous Venipuncture</span>
                  <br>
                  <br>
                  <a href="https://neuling-jpg.github.io/yuchen.github.io/">Yu Chen</a>,
                  Yuxuan Wang,
                  <strong>Bolin Lai</strong>,
                  Zijie Chen,
                  <a href="https://scholar.google.com/citations?user=7BACkU8AAAAJ&hl=en&oi=sra">Xu Cao</a>,
                  <a href="https://ynysjtu.github.io/">Nanyang Ye</a>,
                  Zhongyuan Ren,
                  <a href="http://jakezhao.net/">Junbo Zhao</a>, 
                  <a href="https://xiaoyunzhou27.github.io/xiaoyunzhou/">Xiao-Yun Zhou</a>,
                  <a href="https://nms.kcl.ac.uk/core/?page_id=44">Peng Qi</a>
                  <br>
                  <font color='#A51014'><i>IROS</i>, 2021</font>
                  <br>
                  [<a href="https://arxiv.org/pdf/2105.12945">Paper</a>]
                </td>
              </tr>
              <tr class="pub-item">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/semi_tumor.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">Hetero-Modal Learning and Expansive Consistency Constraints for Semi-Supervised Detection from Multi-Sequence Data</span>
                  <br>
                  <br>
                  <strong>Bolin Lai</strong>, Yuhsuan Wu, 
                  <a href="https://xiaoyunzhou27.github.io/xiaoyunzhou/">Xiao-Yun Zhou</a>,
                  Peng Wang,
                  <a href="https://lelu007.github.io/">Le Lu</a>,
                  Lingyun Huang, Mei Han,
                  <a href="https://scholar.google.com/citations?hl=en&user=mcBd8KUAAAAJ">Jing Xiao</a>,
                  Heping Hu, 
                  <a href="https://extragoya.github.io/">Adam P. Harrison</a>
                  <br>
                  <font color='#A51014'><i>Machine Learning in Medical Imaging</i>, 2021</font>
                  <br>
                  [<a href="https://arxiv.org/pdf/2103.12972.pdf">Paper</a>]
                </td>
              </tr>
              <tr class="pub-item">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/ksp.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">Liver Tumor Localization and Characterization from Multi-phase MR Volumes Using Key-Slice Prediction: A Physician-Inspired Approach</span>
                  <br>
                  <br>
                  <strong>Bolin Lai</strong>*, Yuhsuan Wu*, Xiaoyu Bai*,
                  <a href="https://xiaoyunzhou27.github.io/xiaoyunzhou/">Xiao-Yun Zhou</a>,
                  Peng Wang,
                  <a href="https://jimmycai91.github.io/">Jinzheng Cai</a>,
                  <a href="https://hrlblab.github.io/">Yuankai Huo</a>,
                  Lingyun Huang,
                  <a href="https://jszy.nwpu.edu.cn/en/yongxia.html">Yong Xia</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=mcBd8KUAAAAJ">Jing Xiao</a>,
                  <a href="https://lelu007.github.io/">Le Lu</a>,
                  Heping Hu,
                  <a href="https://extragoya.github.io/">Adam P. Harrison</a>
                  <br>
                  <font color='#A51014'><i>International Workshop on PRedictive Intelligence In MEdicine</i>, 2021</font>
                  <br>
                  [<a href="https://www.researchgate.net/profile/Adam-Harrison-6/publication/354887409_Liver_Tumor_Localization_and_Characterization_from_Multi-phase_MR_Volumes_Using_Key-Slice_Prediction_A_Physician-Inspired_Approach/links/61def5ba4e4aff4a643863ea/Liver-Tumor-Localization-and-Characterization-from-Multi-phase-MR-Volumes-Using-Key-Slice-Prediction-A-Physician-Inspired-Approach.pdf">Paper</a>]
                </td>
              </tr>
              <tr class="pub-item">
                <td style="padding:2%;width:36%;vertical-align:middle">
                  <img src='figures/spinal_dislocation.png' width="100%">
                </td>
                <td style="padding:2%;width:56%;vertical-align:middle">
                  <span class="papertitle">Spatial Regularized Classification Network for Spinal Dislocation Diagnosis</span>
                  <br>
                  <br>
                  <strong>Bolin Lai</strong>, Shiqi Peng, Guangyu Yao,
                  <a href="https://scholar.google.com/citations?user=pbjw9sMAAAAJ&hl=en&oi=ao">Ya Zhang</a>,
                  <a href="https://mediabrain.sjtu.edu.cn/xiaoyun-zhang/">Xiaoyun Zhang</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=x_sgJskAAAAJ">Yanfeng Wang</a>,
                  Hui Zhao
                  <br>
                  <font color='#A51014'><i>Machine Learning in Medical Imaging</i>, 2019</font>
                  <br>
                  [<a href="https://www.researchgate.net/profile/Adam-Harrison-6/publication/354887409_Liver_Tumor_Localization_and_Characterization_from_Multi-phase_MR_Volumes_Using_Key-Slice_Prediction_A_Physician-Inspired_Approach/links/61def5ba4e4aff4a643863ea/Liver-Tumor-Localization-and-Characterization-from-Multi-phase-MR-Volumes-Using-Key-Slice-Prediction-A-Physician-Inspired-Approach.pdf">Paper</a>]
                </td>
              </tr>
            </tbody></table>
          </div>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Awards and Service</h2>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td width="100%" valign="center">
                <ul>
                  <li><a href="https://egovis.github.io/awards/2023_2024/">EgoVis Distinghuished Paper Award</a> @CVPR2025</li>
                  <li><a href="https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee#all-outstanding-reviewer">CVPR Outstanding Reviewer</a> (2025)</li>
                  <li><a href="./figures/eccv_finalist.jpg">ECCV Best Paper Finalist</a> (2024)</li>
                  <li><a href="https://bmvc2022.org/programme/paper-awards/">BMVC Best Student Paper Prize</a> (2022)</li>
                  <li>Outstanding Graduate of SJTU (2020)</li>
                  <li>Taught ECE4871 as a teaching assistant at Georgia Tech in 2021 and 2022.</li>
                  <li>Taught CS7643 Deep Learning as a teaching assistant at Georgia Tech in 2024 and 2025.</li>
                </ul>
                <b>Reviewer for:</b>
                <ul>
                  <li>Computer Vision and Pattern Recognition Conference (CVPR)</li>
                  <li>International Conference on Computer Vision (ICCV)</li>
                  <li>European Conference on Computer Vision (ECCV)</li>
                  <li>Conference on Neural Information Processing Systems (NeurIPS)</li>
                  <li>The Association for Computational Linguistics (ACL)</li>
                  <li>Empirical Methods in Natural Language Processing (EMNLP)</li>
                  <li>Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
                  <li>International Journal of Computer Vision (IJCV)</li>
                  <li>Association for the Advancement of Artificial Intelligence (AAAI)</li>
                  <li>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</li>
                  <li>ACM Multimedia (ACM MM)</li>
                  <li>Journal of Biomedical and Health Informatics (JBHI)</li>
                  <li>IEEE Signal Processing Letters (SPL)</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website is adapted from this <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>

    <!-- JavaScript for Publications Tab Functionality -->
    <script>
      (function() {
        /**
         * Updates the visibility of publication items based on the selected mode
         * @param {string} mode - 'selected' to show only selected publications, 'full' to show all
         */
        function updateVisibility(mode) {
          var allRows = document.querySelectorAll('#publications-section .pub-table tr.pub-item');
          allRows.forEach(function(row) {
            if (mode === 'selected') {
              if (row.classList.contains('selected')) {
                row.classList.remove('is-hidden');
              } else {
                row.classList.add('is-hidden');
              }
            } else {
              row.classList.remove('is-hidden');
            }
          });

          // Handle separator visibility between publication tables
          var tables = document.querySelectorAll('#publications-section .pub-table');
          var separator = document.querySelector('#publications-section .pub-separator');
          if (tables.length > 1 && separator) {
            var secondTableVisibleCount = 0;
            tables[1].querySelectorAll('tr.pub-item').forEach(function(row) {
              if (!row.classList.contains('is-hidden')) secondTableVisibleCount += 1;
            });
            if (mode === 'selected' && secondTableVisibleCount === 0) {
              separator.classList.add('is-hidden');
            } else {
              separator.classList.remove('is-hidden');
            }
          }
        }

        /**
         * Sets the active state of tab buttons
         * @param {string} btnId - The ID of the button to activate
         */
        function setActive(btnId) {
          var selBtn = document.getElementById('tab-selected');
          var fullBtn = document.getElementById('tab-full');
          if (!selBtn || !fullBtn) return;
          if (btnId === 'tab-selected') {
            selBtn.classList.add('active');
            fullBtn.classList.remove('active');
          } else {
            fullBtn.classList.add('active');
            selBtn.classList.remove('active');
          }
        }

        // Initialize publication tab functionality when DOM is loaded
        document.addEventListener('DOMContentLoaded', function() {
          var selectedBtn = document.getElementById('tab-selected');
          var fullBtn = document.getElementById('tab-full');
          if (selectedBtn) {
            selectedBtn.addEventListener('click', function() {
              setActive('tab-selected');
              updateVisibility('selected');
            });
          }
          if (fullBtn) {
            fullBtn.addEventListener('click', function() {
              setActive('tab-full');
              updateVisibility('full');
            });
          }
          setActive('tab-selected');
          updateVisibility('selected');
        });
      })();
    </script>

    <!-- JavaScript for News Section Gradient Effect -->
    <script>
      document.addEventListener('DOMContentLoaded', function() {
        var newsScroll = document.getElementById('news-scroll');
        var newsGradientTop = document.getElementById('news-gradient-top');
        if (newsScroll && newsGradientTop) {
          /**
           * Updates the visibility of the top gradient based on scroll position
           * Shows gradient when scrolled down (hidden content above), hides when at top
           */
          function updateNewsGradient() {
            if (newsScroll.scrollTop > 0) {
              newsGradientTop.style.display = 'block';
            } else {
              newsGradientTop.style.display = 'none';
            }
          }
          newsScroll.addEventListener('scroll', updateNewsGradient);
          // Initial check in case not at top on load
          updateNewsGradient();
        }
      });
    </script>
  </body>
</html>
