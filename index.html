<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Bolin Lai</title>
    <meta name="author" content="Bolin Lai">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:60%;vertical-align:middle">
                <p class="name" style="text-align: center;">Bolin Lai</p>
                <p>Hi! I am a 4th-year PhD student in the Machine Learning Program of Georgia Institute of Technology, advised by <a href="https://rehg.org/">Prof. James Rehg</a> and co-advised by <a href="https://faculty.cc.gatech.edu/~zk15/">Prof. Zsolt Kira</a>. Currently, I'm also a visiting student at CS department of UIUC. Prior to starting my PhD, I got my Master's degree majoring in ECE and Bachelor's degree majoring in Information Engineering from Shanghai Jiao Tong University. I worked with <a href="https://scholar.google.com/citations?user=pbjw9sMAAAAJ&hl=en&oi=ao">Prof. Ya Zhang</a> during my master.</p>
                <p>I was a research scientist intern at GenAI, Meta in 2023 (Llama Image/Video Data Team led by <a href="https://scholar.google.com/citations?user=7v1LZxUAAAAJ&hl=en">Guan Pang</a>) and 2024 (Llama Applied Multi-modal Team led by <a href="https://n-zhang.github.io/">Ning Zhang</a>). I closely worked with <a href="https://aptx4869lm.github.io/">Miao Liu</a>, <a href="http://xiaotong.me/">Tong Xiao</a>, <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a> and <a href="https://www.lawrencechen.me/">Lawrence Chen</a> on generative model projects.</p>
                <p style="text-align:center">
                  <a href="mailto:bolin.lai@gatech.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=lWrljmQAAAAJ&hl=en"> Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/BolinLai">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/bolin-lai-625b78139">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/bryanislucky">Twitter</a>
                </p>
              </td>
              <td style="padding:2.5%;width:35%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit:cover;" alt="profile photo" src="figures/photo.png">
                <!-- <img style="width:100%;max-width:100%;object-fit:cover;border-radius: 50%;" alt="profile photo" src="figures/photo.png"> -->
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
              </td>
            </tr>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <p>
                  My research interests lie in <b>Multimodal Learning</b>, <b>Generative Models</b> (including <b>Multimodal LLMs</b> and <b>Diffusion Models</b>) and <b>Video Understanding</b>. 
                  <!-- Currently, I'm focusing on joint representation learning of vision and language in videos as well as image/video generation using diffusion models. -->
                  Currently, I'm focusing on advancing multimodal unerstanding and generation through the integration of architectures/features of Large Language Models (LLMs) and Diffusion Models (DMs), aiming to connect and leverage the latent representation spaces of the two model architectures.
                </p>
                <p><font color="#FF8080">I'm looking for self-motivated graduate/ungraduate students to collaborate with. Don't hesitate to reach out to me if you are interested.</font></p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>News</h2>
              </td>
            </tr>
            <tr>
              <td style="padding:5px;width:100%;vertical-align:middle">
                <ul>
                  <li>Mar 2025: üéâ I successfully passed my thesis proposal. Thank all committee members (<a href="https://rehg.org/">James Rehg</a>, <a href="https://faculty.cc.gatech.edu/~zk15/">Zsolt Kira</a>, <a href="https://faculty.cc.gatech.edu/~hays/">James Hays</a>, <a href="https://faculty.cc.gatech.edu/~judy/">Judy Hoffman</a>)! I'll be on the job market in September of 2025. üë®‚Äçüíª‚Äç</li>
                  <li>Mar 2025: I will be interning at Meta GenAI at Seattle this summer working with Dr. <a href="https://imisra.github.io/">Ishan Misra</a>.</li>
                  <li><font color="#fb8b23">Feb 2025: üéâ I have one first-author paper (<a href="https://bolinlai.github.io/projects/InstaManip/">InstaManip</a>) and two co-author papers (<a href="https://arxiv.org/pdf/2501.04336">VideoMindPalace</a> and <a href="https://arxiv.org/pdf/2504.02244">SocialGesture</a>) accepted by CVPR 2025. Thank all collaborators. See you @Nashville.</font></li>
                  <li>Oct 2024: üîç We released a thorough <a href="https://arxiv.org/pdf/2410.14045">survey</a> in action anticipation. Please check out if you are interested in this field.</li>
                  <li>Oct 2024: üèÖ Our <a href="https://bolinlai.github.io/Lego_EgoActGen/">LEGO</a> paper was nominated in the <a href="./figures/eccv_finalist.jpg">Best Paper Finalist</a> @ECCV2024. Congratulations to all co-authors!</li>
                  <li>Aug 2024: üé§ Our LEGO paper got Oral presentation.</li>
                  <li>July 2024: üéâ Two first-author papers were accepted by ECCV! Please check out our latest work: <a href="https://bolinlai.github.io/Lego_EgoActGen/">LEGO</a> (for action generation) and <a href="https://bolinlai.github.io/CSTS-EgoGazeAnticipation/">CSTS</a> (for gaze forecasting). Thank all the co-authors!</li>
                  <li>May 2024: I started my second intenrship at GenAI, Meta in Bay Area.</li>
                  <li>Mar 2024: One co-author paper was accepted by CVPR (Oral). See you in Seattle!</li>
                  <!-- <li>Jul 2023: Our expansion of prior work GLC was accepted by IJCV!</li> -->
                  <!-- <li>May 2023: I started my internship at GenAI Meta in Bay Area!</li>
                  <li>Apr 2023: I successfully passed the qualifying exam.</li>
                  <li>Mar 2023: One paper was accepted to the Findings of ACL2023. Please check out our new dataset for social understanding: <a href="https://aclanthology.org/2023.findings-acl.411.pdf">Werewolf Among Us</a>.</li> -->
                  <!-- <li>Nov 2022: We won the Best Student Paper Prize on BMVC. Thanks to all co-authors!</li>
                  <li>Sep 2022: Our work <a href="https://bolinlai.github.io/GLC-EgoGazeEst/">GLC</a> was accepted by <a href="https://bmvc2022.org/">BMVC 2022</a>!</li>
                  <li>Jan 2022: I started working with <a href="https://rehg.org/">Prof. James Rehg</a> at Georgia Tech.</li> -->
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle;">
                <img src='figures/flexti2v.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle;">
                <span class="papertitle">Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>,
                <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                <a href="https://www.irohxucao.com/">Xu Cao</a>,
                <a href="https://ryanxli.github.io/">Xiang Li</a>,
                <a href="https://rehg.org/">James M. Rehg</a>,
                <br>
                <font color='#A51014'><em>Under Review</em>, 2025</font>
                <br>
                <a href="https://bolinlai.github.io/projects/FelxTI2V/">Webpage</a> /
                <a href="">Paper</a> /
                <a href="https://github.com/BolinLai/FlexTI2V">Code</a>
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle;">
                <img src='figures/instamanip.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle;">
                <span class="papertitle">Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>,
                <a href="https://xujuefei.com/">Felix Juefei-Xu</a>,
                <a href="https://aptx4869lm.github.io/">Miao Liu</a>,
                <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a>,
                <a href="https://hockeybro12.github.io/">Nikhil Mehta</a>,
                <a href="https://cs.stanford.edu/~cgzhu/">Chenguang Zhu</a>,
                <a href="https://oodbag.github.io/">Zeyi Huang</a>,
                <a href="https://rehg.org/">James M. Rehg</a>,
                <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                <a href="https://n-zhang.github.io/">Ning Zhang</a>,
                <a href="http://xiaotong.me/">Tong Xiao</a>
                <br>
                <font color='#A51014'><em>CVPR</em>, 2025 (Highlight)</font>
                <br>
                <a href="https://bolinlai.github.io/projects/InstaManip/">Webpage</a> /
                <a href="https://arxiv.org/pdf/2412.01027">Paper</a> /
                <a href="https://github.com/BolinLai/InstaManip">Code</a> /
                <a href="https://huggingface.co/bolinlai/InstaManip">HuggingFace</a> /
                <a href="https://www.youtube.com/watch?v=JedJi_f-oQk">Video</a> /
                <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32621.png?t=1748048756.2214794">Poster</a>
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle;">
                <img src='figures/videomindpalace.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle;">
                <span class="papertitle">Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs</span>
                <br>
                <br>
                <a href="https://oodbag.github.io/">Zeyi Huang</a>,
                Yuyang Ji,
                <a href="https://minione.github.io/">Xiaofang Wang</a>,
                <a href="https://hockeybro12.github.io/">Nikhil Mehta</a>,
                <a href="http://xiaotong.me/">Tong Xiao</a>,
                Donghyun Lee,
                <a href="https://www.linkedin.com/in/siggyv/">Sigmund VanValkenburgh</a>,
                <a href="https://scholar.google.com/citations?user=QRvXHNsAAAAJ&hl=en">Shengxin Zha</a>,
                <strong>Bolin Lai</strong>,
                <a href="https://lichengunc.github.io/">Licheng Yu</a>,
                <a href="https://n-zhang.github.io/">Ning Zhang</a>,
                <a href="https://pages.cs.wisc.edu/~yongjaelee/"> Yong Jae Lee</a>,
                <a href="https://aptx4869lm.github.io/">Miao Liu</a>
                <br>
                <font color='#A51014'><em>CVPR</em>, 2025</font>
                <br>
                [<a href="https://arxiv.org/pdf/2501.04336">Paper</a>]
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle;">
                <img src='figures/socialgesture.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle;">
                <span class="papertitle">SocialGesture: Delving into Multi-person Gesture Understanding</span>
                <br>
                <br>
                <a href="https://www.irohxucao.com/">Xu Cao</a>,
                Pranav Virupaksha,
                <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                <strong>Bolin Lai</strong>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                <a href="https://rehg.org/">James M. Rehg</a>
                <br>
                <font color='#A51014'><em>CVPR</em>, 2025</font>
                <br>
                <a href="https://arxiv.org/pdf/2504.02244">Paper</a> /
                <a href="https://huggingface.co/datasets/IrohXu/SocialGesture">Dataset</a>
              </td>
            </tr>

            
            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle;">
                <img src='figures/action_forecasting_survey.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle;">
                <span class="papertitle">Human Action Anticipation: A Survey</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>*,
                <a href="http://www.qxcv.net/research/">Sam Toyer*</a>,
                <a href="https://tushar-n.github.io/">Tushar Nagarajan</a>,
                <a href="http://rohitgirdhar.github.io/">Rohit Girdhar</a>,
                <a href="https://scholar.google.com/citations?user=QRvXHNsAAAAJ&hl=en&oi=ao">Shengxin Zha</a>,
                <a href="https://rehg.org/">James M. Rehg</a>,
                <a href="http://www.cs.cmu.edu/~kkitani">Kris Kitani</a>,
                <a href="http://www.cs.utexas.edu/~grauman/">Kristen Grauman</a>,
                <a href="https://rutadesai.github.io/">Ruta Desai</a>,
                <a href="https://aptx4869lm.github.io/">Miao Liu</a>
                <br>
                <font color='#A51014'><em>Preprint</em>, 2024</font>
                <br>
                [<a href="https://arxiv.org/pdf/2410.14045">Paper</a>]
              </td>
            </tr>

            <tr></tr>
              <td style="padding:2%;width:36%;vertical-align:middle;">
                <img src='figures/vcog-bench.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle;">
                <span class="papertitle">What is the Visual Cognition Gap between Humans and Multimodal LLMs?</span>
                <br>
                <br>
                <a href="https://www.irohxucao.com/">Xu Cao</a>, 
                <strong>Bolin Lai</strong>,
                <a href="https://wenqian-ye.github.io/">Wenqian Ye</a>,
                <a href="https://ysma.me/">Yunsheng Ma</a>,
                <a href="https://scholar.google.com/citations?user=mNxy4CkAAAAJ&hl=en&oi=sra">Joerg Heintz</a>,
                <a href="https://whatashot.github.io/">Jintai Chen</a>,
                <a href="https://scholar.google.com/citations?user=dPPOpHUAAAAJ&hl=en&oi=sra">Jianguo Cao</a>,
                <a href="https://rehg.org/">James M. Rehg</a>
                <br>
                <font color='#A51014'><em>Under Review</em>, 2024</font>
                <br>
                [<a href="https://arxiv.org/pdf/2406.10424">Paper</a>]
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle;">
                <img src='figures/social_ai_survey.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle;">
                <span class="papertitle">Towards Social AI: A Survey on Understanding Social Interactions</span>
                <br>
                <br>
                <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                <a href="https://yocodeyo.github.io/">Minzhi Li</a>,
                <strong>Bolin Lai</strong>,
                <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://www.irohxucao.com/">Xu Cao</a>, 
                <a href="http://karaozgur.com/">Ozgur Kara</a>,
                <a href="https://scholar.google.com/citations?user=e4jgsUcAAAAJ&hl=en">Bikram Boote</a>,
                <a href="http://wyshi.github.io/">Weiyan Shi</a>,
                <a href="http://www.diyiyang.com/">Diyi Yang</a>,
                <a href="https://rehg.org/">James M. Rehg</a>
                <br>
                <font color='#A51014'><em>Under Review of TPAMI</em>, 2024</font>
                <br>
                [<a href="https://arxiv.org/pdf/2409.15316">Paper</a>]
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle;">
                <img src='figures/lego.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle;">
                <span class="papertitle">LEGO: <u>L</u>earning <u>EGO</u>centric Action Frame Generation via Visual Instruction Tuning</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>,
                <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a>,
                <a href="https://www.lawrencechen.me/">Lawrence Chen</a>,
                <a href="https://scholar.google.com/citations?user=7v1LZxUAAAAJ&hl=en">Guan Pang</a>,
                <a href="https://rehg.org/">James M. Rehg</a>,
                <a href="https://aptx4869lm.github.io/">Miao Liu</a>
                <br>
                <font color='#A51014'><em>ECCV</em>, 2024 (Oral, Best Paper Finalist)</font>
                <br>
                <a href="https://bolinlai.github.io/Lego_EgoActGen/">Webpage</a> /
                <a href="https://arxiv.org/pdf/2312.03849.pdf">Paper</a> /
                <a href="https://github.com/BolinLai/LEGO">Code</a> /
                <a href="https://huggingface.co/datasets/bolinlai/LEGO-Dataset">Dataset</a> /
                <a href="https://huggingface.co/collections/bolinlai/lego-67b386cf642909c56776f754">HuggingFace</a> /
                <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01383-supp.pdf">Supplementary</a> /
                <a href="https://www.youtube.com/watch?v=90bNgwe4URU">Video</a> /
                <a href="https://eccv2024.ecva.net/media/PosterPDFs/ECCV%202024/1257.png?t=1726963913.2047732">Poster</a> /
                <a href="https://www.cc.gatech.edu/news/new-generative-tool-provides-images-accompany-step-step-instructions">Press: GT News</a>
              </td>
            </tr>
            
            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle;">
                <img src='figures/avgaze.png' width="90%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle;">
                <span class="papertitle">Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                <a href="https://aptx4869lm.github.io/">Miao Liu*</a>,
                <a href="https://rehg.org/">James M. Rehg*</a>
                <br>
                <font color='#A51014'><em>ECCV</em>, 2024</font>
                <br>
                <a href="https://bolinlai.github.io/CSTS-EgoGazeAnticipation/">Webpage</a> /
                <a href="https://arxiv.org/pdf/2305.03907.pdf">Paper</a> /
                <a href="https://github.com/BolinLai/CSTS">Code</a> / 
                <a href="https://github.com/BolinLai/CSTS/tree/main/data">Data Split</a> /
                <a href="https://huggingface.co/bolinlai/CSTS">HuggingFace</a> /
                <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01396-supp.pdf">Supplementary</a> /
                <a href="https://www.youtube.com/watch?v=5ApMaLhlxUc">Video</a> /
                <a href="https://eccv2024.ecva.net/media/PosterPDFs/ECCV%202024/268.png?t=1726967693.823367">Poster</a>
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle">
                <img src='figures/multimodal_social.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle">
                <span class="papertitle">Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations</span>
                <br>
                <br>
                <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                <strong>Bolin Lai</strong>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://scholar.google.com/citations?user=e4jgsUcAAAAJ&hl=en">Bikram Boote</a>,
                <a href="https://rehg.org/">James M. Rehg</a>
                <br>
                <font color='#A51014'><em>CVPR</em>, 2024 (Oral) [Acceptance Rate 0.8%]</font>
                <br>
                <a href="https://sangmin-git.github.io/projects/MMSI/">Webpage</a> /
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Modeling_Multimodal_Social_Interactions_New_Challenges_and_Baselines_with_Densely_CVPR_2024_paper.pdf">Paper</a> /
                <a href="https://github.com/sangmin-git/MMSI">Code</a> /
                <a href="https://www.dropbox.com/scl/fo/fbv6njzu1ynbgv9wgtrwo/ANPk2TKqK2rl44MqKu05ogk?rlkey=yx7bmzmmiymauvz99q2rvjajg&st=305631zj&dl=0">Split & Annotations</a> /
                <a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lee_Modeling_Multimodal_Social_CVPR_2024_supplemental.pdf">Supplementary</a>
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle">
                <img src='figures/deduction.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle">
                <span class="papertitle">Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>*,
                <a href="https://icefoxzhx.github.io/">Hongxin Zhang*</a>,
                <a href="https://aptx4869lm.github.io/">Miao Liu*</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=EnC_6s0AAAAJ">Aryan Pariani*</a>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                <a href="https://www.shirley.id/">Shirley Anugrah Hayati</a>,
                <a href="https://rehg.org/">James M. Rehg</a>,
                <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
                <br>
                <font color='#A51014'><em>ACL Findings</em>, 2023</font>
                <br>
                <a href="https://bolinlai.github.io/projects/Werewolf-Among-Us/">Webpage</a> /
                <a href="https://aclanthology.org/2023.findings-acl.411.pdf">Paper</a> /
                <a href="https://github.com/SALT-NLP/PersuationGames">Code</a> /
                <a href="https://huggingface.co/datasets/bolinlai/Werewolf-Among-Us">Dataset</a> /
                <a href="https://www.youtube.com/watch?v=Uch5sUwDs2w">Video</a>
                <!-- <p>Modeling social behaviors of multi-person scenarios using both video and language.</p> -->
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle">
                <img src='figures/gaze_est.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle">
                <span class="papertitle">In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation and Beyond</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>,
                <a href="https://aptx4869lm.github.io/">Miao Liu</a>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://rehg.org/">James M. Rehg</a>
                <br>
                <font color='#A51014'><em>International Journal of Computer Vision (IJCV)</em>, 2023</font>
                <br>
                <a href="https://bolinlai.github.io/GLC-EgoGazeEst/">Webpage</a> /
                <a href="https://link.springer.com/article/10.1007/s11263-023-01879-7">Paper</a> /
                <a href="https://github.com/BolinLai/GLC">Code</a>
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle">
                <img src='figures/glc.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle">
                <span class="papertitle">In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>,
                <a href="https://aptx4869lm.github.io/">Miao Liu</a>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://rehg.org/">James M. Rehg</a>
                <br>
                <font color='#A51014'><em>BMVC</em>, 2022 (Spotlight, Best Student Paper)</font>
                <br>
                <a href="https://bolinlai.github.io/GLC-EgoGazeEst/">Webpage</a> /
                <a href="https://bmvc2022.mpi-inf.mpg.de/0227.pdf">Paper</a> /
                <a href="https://github.com/BolinLai/GLC">Code</a> /
                <a href="https://bolinlai.github.io/GLC-EgoGazeEst/Ego4D_Gaze_Split.zip">Data Split</a> /
                <a href="https://bmvc2022.mpi-inf.mpg.de/0227_supp.zip">Supplementary</a> /
                <a href="https://bmvc2022.mpi-inf.mpg.de/0227_video.mp4">Video</a> /
                <a href="https://bmvc2022.mpi-inf.mpg.de/0227_poster.pdf">Poster</a>
                <!-- <p>Improving egocentric gaze estimation by explicitly modeling global-local correlations in transformer-based architecture.</p> -->
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  ---------------- Research before my PhD, mainly about medical image analysis ----------------
                </td>
              </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle">
                <img src='figures/venibot.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle">
                <span class="papertitle">Semi-supervised Vein Segmentation of Ultrasound Images for Autonomous Venipuncture</span>
                <br>
                <br>
                <a href="https://neuling-jpg.github.io/yuchen.github.io/">Yu Chen</a>,
                Yuxuan Wang,
                <strong>Bolin Lai</strong>,
                Zijie Chen,
                <a href="https://scholar.google.com/citations?user=7BACkU8AAAAJ&hl=en&oi=sra">Xu Cao</a>,
                <a href="https://ynysjtu.github.io/">Nanyang Ye</a>,
                Zhongyuan Ren,
                <a href="http://jakezhao.net/">Junbo Zhao</a>, 
                <a href="https://xiaoyunzhou27.github.io/xiaoyunzhou/">Xiao-Yun Zhou</a>,
                <a href="https://nms.kcl.ac.uk/core/?page_id=44">Peng Qi</a>
                <br>
                <font color='#A51014'><em>IROS</em>, 2021</font>
                <br>
                [<a href="https://arxiv.org/pdf/2105.12945">Paper</a>]
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle">
                <img src='figures/semi_tumor.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle">
                <span class="papertitle">Hetero-Modal Learning and Expansive Consistency Constraints for Semi-Supervised Detection from Multi-Sequence Data</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>, Yuhsuan Wu, 
                <a href="https://xiaoyunzhou27.github.io/xiaoyunzhou/">Xiao-Yun Zhou</a>,
                Peng Wang,
                <a href="https://lelu007.github.io/">Le Lu</a>,
                Lingyun Huang, Mei Han,
                <a href="https://scholar.google.com/citations?hl=en&user=mcBd8KUAAAAJ">Jing Xiao</a>,
                Heping Hu, 
                <a href="https://extragoya.github.io/">Adam P. Harrison</a>
                <br>
                <font color='#A51014'><em>Machine Learning in Medical Imaging</em>, 2021</font>
                <br>
                [<a href="https://arxiv.org/pdf/2103.12972.pdf">Paper</a>]
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle">
                <img src='figures/ksp.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle">
                <span class="papertitle">Liver Tumor Localization and Characterization from Multi-phase MR Volumes Using Key-Slice Prediction: A Physician-Inspired Approach</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>*, Yuhsuan Wu*, Xiaoyu Bai*,
                <a href="https://xiaoyunzhou27.github.io/xiaoyunzhou/">Xiao-Yun Zhou</a>,
                Peng Wang,
                <a href="https://jimmycai91.github.io/">Jinzheng Cai</a>,
                <a href="https://hrlblab.github.io/">Yuankai Huo</a>,
                Lingyun Huang,
                <a href="https://jszy.nwpu.edu.cn/en/yongxia.html">Yong Xia</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=mcBd8KUAAAAJ">Jing Xiao</a>,
                <a href="https://lelu007.github.io/">Le Lu</a>,
                Heping Hu,
                <a href="https://extragoya.github.io/">Adam P. Harrison</a>
                <br>
                <font color='#A51014'><em>International Workshop on PRedictive Intelligence In MEdicine</em>, 2021</font>
                <br>
                [<a href="https://www.researchgate.net/profile/Adam-Harrison-6/publication/354887409_Liver_Tumor_Localization_and_Characterization_from_Multi-phase_MR_Volumes_Using_Key-Slice_Prediction_A_Physician-Inspired_Approach/links/61def5ba4e4aff4a643863ea/Liver-Tumor-Localization-and-Characterization-from-Multi-phase-MR-Volumes-Using-Key-Slice-Prediction-A-Physician-Inspired-Approach.pdf">Paper</a>]
              </td>
            </tr>

            <tr>
              <td style="padding:2%;width:36%;vertical-align:middle">
                <img src='figures/spinal_dislocation.png' width="100%">
              </td>
              <td style="padding:2%;width:56%;vertical-align:middle">
                <span class="papertitle">Spatial Regularized Classification Network for Spinal Dislocation Diagnosis</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>, Shiqi Peng, Guangyu Yao,
                <a href="https://scholar.google.com/citations?user=pbjw9sMAAAAJ&hl=en&oi=ao">Ya Zhang</a>,
                <a href="https://mediabrain.sjtu.edu.cn/xiaoyun-zhang/">Xiaoyun Zhang</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=x_sgJskAAAAJ">Yanfeng Wang</a>,
                Hui Zhao
                <br>
                <font color='#A51014'><em>Machine Learning in Medical Imaging</em>, 2019</font>
                <br>
                [<a href="https://www.researchgate.net/profile/Adam-Harrison-6/publication/354887409_Liver_Tumor_Localization_and_Characterization_from_Multi-phase_MR_Volumes_Using_Key-Slice_Prediction_A_Physician-Inspired_Approach/links/61def5ba4e4aff4a643863ea/Liver-Tumor-Localization-and-Characterization-from-Multi-phase-MR-Volumes-Using-Key-Slice-Prediction-A-Physician-Inspired-Approach.pdf">Paper</a>]
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Service</h2>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td width="100%" valign="center">
                Reviewer for <br>
                - Computer Vision and Pattern Recognition Conference (CVPR) <br>
                - International Conference on Computer Vision (ICCV) <br>
                - European Conference on Computer Vision (ECCV) <br>
                - The Association for Computational Linguistics (ACL) <br>
                - Empirical Methods in Natural Language Processing (EMNLP) <br>
                - International Journal of Computer Vision (IJCV) <br>
                - Association for the Advancement of Artificial Intelligence (AAAI) <br>
                - International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) <br>
                - Journal of Biomedical and Health Informatics (JBHI) <br>
                - IEEE Signal Processing Letters (SPL)
                <br>
                <br>
                Taught ECE4871 as a teacher assistant at Georgia Tech in 2021 and 2022. <br>
                Taught CS7643 Deep Learning as a teacher assistant at Georgia Tech in 2024 and 2025.
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website is adapted from this <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>
