<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games">
  <meta name="keywords" content="social deduction game, social ai, mutlimodal learning, dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games</title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/gt_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
  <style>
    table {
      font-family: Arial, sans-serif;
      border-collapse: collapse;
      width: 100%;
    }
    td, th {
      border: 1px solid #dddddd;
      text-align: left;
      padding: 8px;
    }
    th {
      background-color: #dddddd;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games</h1>
          <div class="is-size-4">
            <span style="font-size: 30px; color:darkred">ACL Findings 2023</span>
          </div>
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://bolinlai.github.io/">Bolin Lai*</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://icefoxzhx.github.io/">Hongxin Zhang*</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://aptx4869lm.github.io/">Miao Liu*</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=EnC_6s0AAAAJ">Aryan Pariani*</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://fkryan.github.io/">Fiona Ryan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://vjwq.github.io/">Wenqi Jia</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.shirley.id/">Shirley Anugrah Hayati</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://rehg.org/">James M. Rehg</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a><sup>5</sup>,
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Georgia Institute of Technology</span>&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Shanghai Jiao Tong University</span>&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup>Meta AI</span><br>
            <span class="author-block"><sup>4</sup>University of Minnesota</span>&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>5</sup>Stanford University</span>
          </div>
          <div class="column has-text-centered">
            <span class="link-block">
              <a href="https://aclanthology.org/2023.findings-acl.411.pdf" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://github.com/SALT-NLP/PersuationGames" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://huggingface.co/datasets/bolinlai/Werewolf-Among-Us" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fa fa-database" aria-hidden="true"></i></span>
                <span>Dataset</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <span style="font-size: 20px; color:darkorange">We have released new follow-up work with more annotations. More details can be found here: <br>
            1. <a href="https://sangmin-git.github.io/projects/MMSI/">Modeling Multimodal Social Interactions</a> (CVPR 2024, Oral) <br>
            2. <a href="https://www.irohxucao.com/SocialGesture/">Social Gesture</a> (CVPR 2025) <br>
            3. <a href="https://arxiv.org/pdf/2503.19851">Towards Online Multi-Modal Social Interaction Understanding</a> (Under Review)
          </span>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
        <img src="figures/teaser.png"  style="width:100%;">
      </center>
    </div>
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
          <div class="content has-text-justified">
            <p>
               Demonstration of the six persuasion strategies included in our dataset and the corresponding video. 
               The players are numbered as 1,2,3,4 from left to right. 
               Players’ roles might be changed during the game. 
               In this example, player1’s and player2’s cards were swapped by the troublemaker, and player3’s and player4’s cards were swapped by the robber. 
               Player 2 voted for player 3 at the end while the others voted for player 2.
            </p>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- End teaser -->


<!-- Video-->
<section class="section">
  <div class="container is-max-desktop">
      <div class="iframe-container">
        <iframe
            src="https://www.youtube.com/embed/Uch5sUwDs2w?si=Kpl1dbzbp8wGHNQa"
            title="YouTube video player"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            referrerpolicy="strict-origin-when-cross-origin"
            allowfullscreen>
        </iframe>
      </div>
  </div>
</section>
<!-- End video -->


<section class="section">
  <div class="container is-max-desktop">
    <center><h2 class="title is-3">Abstract</h2></center>
    <br>
    <div class="columns is-centered has-text-centered">
      <div class="content has-text-justified">
        <p>
          Persuasion modeling is a key building block for conversational agents. 
          Existing works in this direction are limited to analyzing textual dialogue corpora. 
          We argue that visual signals also play an important role in understanding human persuasive behaviors. 
          In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. 
          Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26,647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. 
          We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. 
          We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. 
        </p>
      </div>
    </div>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">

        <center><h2 class="title is-3">Annotations of Persuasion Strategies</h2></center>
        <div class="image">
          <center>
            <img src="figures/label.png"  style="width:100%;">
          </center>
        </div>
        <div class="caption">
          <div class="columns is-centered has-text-centered">
            <div class="content has-text-justified">
              <p>
                Utterance-level persuasion strategy annotations. 
                AUL refers to the average utterance length in terms of the number of words in an utterance and `\alpha` refers to Krippendorff’s alpha.
              </p>
            </div>
          </div>
          <hr>
        </div>
        
        <center><h2 class="title is-3">Model Architecture</h2></center>
        <div class="image">
          <center>
            <img src="figures/method.png"  style="width:50%;">
          </center>
        </div>
        <div class="caption">
          <div class="columns is-centered has-text-centered">
            <div class="content has-text-justified">
              <p>
                Architecture of the independent model for each strategy. 
                We fix the parameters in the video encoder and train the other modules end-to-end. 
                `\oplus` denotes the concatenation of two feature representations
              </p>
            </div>
          </div>
          <hr>
        </div>

        <center><h2 class="title is-3">Experiments</h2></center>
        <div class="image">
          <center>
            <img src="figures/experiments.png"  style="width:100%;">
          </center>
        </div>
        <div class="caption">
          <div class="columns is-centered has-text-centered">
            <div class="content has-text-justified">
              <p>
                Experimental Results on incorporating visual features for persuasion strategy prediction. 
                We train an independent model for each category using BERT and RoBERTa backbones. 
                Additionally, we also use the off-theshelf Multi-Task BERT model (MT-BERT) (Chawla et al., 2021) to jointly predict all categories.
              </p>
            </div>
          </div>
          <hr>
        </div>

        <center><h2 class="title is-3">Ablation Study of Context Length</h2></center>
        <div class="image">
          <center>
            <img src="figures/ablation.png"  style="width:100%;">
          </center>
        </div>
        <div class="caption">
          <div class="columns is-centered has-text-centered">
            <div class="content has-text-justified">
              <p>
                Ablation study of adopting different context lengths for persuasion strategy prediction.
              </p>
            </div>
          </div>
          <hr>
        </div>

        <center><h2 class="title is-3">Domain Generalization</h2></center>
        <div class="image">
          <center>
            <img src="figures/generalization.png"  style="width:100%;">
          </center>
        </div>
        <div class="caption">
          <div class="columns is-centered has-text-centered">
            <div class="content has-text-justified">
              <p>
                Data domain generalization experiments. 
                We report the testing performance on the Ego4D dataset using models trained only on YouTube data (w.o. Fine-tuning), and trained on YouTube data and further fine-tuned with Ego4D data (w. Fine-tuning). 
                We also report the performance of the models trained only on Ego4D dataset (Ego4D Only) as comparison.
              </p>
            </div>
          </div>
          <hr>
        </div>

        <center><h2 class="title is-3">SVM Weights Visualization for Vote Prediction</h2></center>
        <div class="image">
          <center>
            <img src="figures/weight_vis.png"  style="width:70%;">
          </center>
        </div>
        <div class="caption">
          <div class="columns is-centered has-text-centered">
            <div class="content has-text-justified">
              <p>
                Weights visualization of persuasion strategies in logistic regression. 
                The connection between a strategy and 0 means this strategy contributes to the prediction of 0 (i.e. the voter doesn’t vote for the candidate). 
                Likewise, the connection between a strategy and 1 denotes this strategy contributes to the prediction of 1 (i.e. the voter votes for the candidate). 
                The transparency of lines corresponds to the weights of logistic regression. 
                A less transparent line suggests a greater weight and more impact on the output.
              </p>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <center><h2 class="title">BibTeX</h2></center>
    <pre><code>@inproceedings{lai2023werewolf,
      title={Werewolf among us: Multimodal resources for modeling persuasion behaviors in social deduction games},
      author={Lai, Bolin and Zhang, Hongxin and Liu, Miao and Pariani, Aryan and Ryan, Fiona and Jia, Wenqi and Hayati, Shirley Anugrah and Rehg, James and Yang, Diyi},
      booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
      pages={6570--6588},
      year={2023}}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            We appreciate the original source code from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
