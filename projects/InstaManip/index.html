<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation">
  <meta name="keywords" content="image manipulation, in-context learning, autoregressive model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation</title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/gt_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    table {
      font-family: Arial, sans-serif;
      border-collapse: collapse;
      width: 100%;
    }
    td, th {
      border: 1px solid #dddddd;
      text-align: left;
      padding: 8px;
    }
    th {
      background-color: #dddddd;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation</h1>
          <div class="is-size-4 publication-conference">
            <span class="conference-block" style="font-size: 30px; color:darkred">CVPR 2025 (Highlight)</span>
          </div>
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://bolinlai.github.io/">Bolin Lai</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://xujuefei.com/">Felix Juefei-Xu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://aptx4869lm.github.io/">Miao Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://hockeybro12.github.io/">Nikhil Mehta</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cs.stanford.edu/~cgzhu/">Chenguang Zhu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://oodbag.github.io/">Zeyi Huang</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://rehg.org/">James M. Rehg</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://n-zhang.github.io/">Ning Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://xiaotong.me/">Tong Xiao</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>GenAI, Meta</span>&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Georgia Institute of Technology</span>&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup>University of Illinois Urbana-Champaign</span>
            <span class="author-block"><sup>4</sup>Sungkyunkwan University</span>&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>5</sup>University of Wisconsin-Madison</span>
          </div>
          <!-- <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Work partially done during an internship at Meta GenAI.</span>
          </div> -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2412.01027" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Supplementary</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://github.com/BolinLai/InstaManip" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/bolinlai/InstaManip" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><svg class="svg-inline--fa fa-face-smiling-hands" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="face-smiling-hands" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M411.1 495.3C382.8 506.1 352.1 512 319.1 512C287.9 512 257.2 506.1 228.9 495.3C245.9 473.7 255.1 446.4 255.1 416.8V386C274.1 394.4 295.4 400 319.1 400C344.6 400 365.9 394.4 384 386V416.8C384 446.4 394.1 473.7 411.1 495.3V495.3zM575.7 242.6C558.8 236.8 539.5 240.6 526.1 254.1L478.1 301.1C469.6 287.4 453.9 278.4 436 278.4C407.3 278.4 384 301.7 384 330.4V349.6C367.2 360.3 345.9 368 319.1 368C294.1 368 272.8 360.3 255.1 349.6V330.4C255.1 301.7 232.7 278.4 203.1 278.4C186.1 278.4 170.4 287.4 161 301.1L113.9 254.1C100.5 240.6 81.15 236.8 64.34 242.6C71.31 107.5 183.1 0 319.1 0C456.9 0 568.7 107.5 575.7 242.6V242.6zM281.6 228.8C283.7 231.6 287.3 232.7 290.5 231.6C293.8 230.5 295.1 227.4 295.1 224C295.1 206.1 289.3 188.4 279.4 175.2C269.6 162.2 255.5 152 239.1 152C224.5 152 210.4 162.2 200.6 175.2C190.7 188.4 183.1 206.1 183.1 224C183.1 227.4 186.2 230.5 189.5 231.6C192.7 232.7 196.3 231.6 198.4 228.8L198.4 228.8L198.6 228.5C198.8 228.3 198.1 228 199.3 227.6C199.1 226.8 200.9 225.7 202.1 224.3C204.6 221.4 208.1 217.7 212.3 213.1C221.1 206.2 231.2 200 239.1 200C248.8 200 258.9 206.2 267.7 213.1C271.9 217.7 275.4 221.4 277.9 224.3C279.1 225.7 280 226.8 280.7 227.6C281 228 281.2 228.3 281.4 228.5L281.6 228.8L281.6 228.8zM450.5 231.6C453.8 230.5 456 227.4 456 224C456 206.1 449.3 188.4 439.4 175.2C429.6 162.2 415.5 152 400 152C384.5 152 370.4 162.2 360.6 175.2C350.7 188.4 344 206.1 344 224C344 227.4 346.2 230.5 349.5 231.6C352.7 232.7 356.3 231.6 358.4 228.8L358.4 228.8L358.6 228.5C358.8 228.3 358.1 228 359.3 227.6C359.1 226.8 360.9 225.7 362.1 224.3C364.6 221.4 368.1 217.7 372.3 213.1C381.1 206.2 391.2 200 400 200C408.8 200 418.9 206.2 427.7 213.1C431.9 217.7 435.4 221.4 437.9 224.3C439.1 225.7 440 226.8 440.7 227.6C441 228 441.2 228.3 441.4 228.5L441.6 228.8L441.6 228.8C443.7 231.6 447.3 232.7 450.5 231.6V231.6zM68.69 299.3C62.44 293.1 62.44 282.9 68.69 276.7C74.93 270.4 85.06 270.4 91.31 276.7L170.3 355.7C175.4 360.8 184 357.2 184 350.1V330.4C184 319.4 192.1 310.4 204 310.4C215 310.4 224 319.4 224 330.4V416.8C224 469.4 181.4 512 128.8 512C103.6 512 79.34 501.1 61.49 484.1L4.686 427.3C-1.562 421.1-1.562 410.9 4.686 404.7C10.93 398.4 21.07 398.4 27.31 404.7L46.63 424C49.22 426.6 53.41 426.6 55.1 424C58.59 421.4 58.59 417.2 55.1 414.6L4.686 363.3C-1.562 357.1-1.562 346.9 4.686 340.7C10.93 334.4 21.07 334.4 27.31 340.7L78.63 392C81.22 394.6 85.41 394.6 87.1 392C90.59 389.4 90.59 385.2 87.1 382.6L20.69 315.3C14.44 309.1 14.44 298.9 20.69 292.7C26.93 286.4 37.06 286.4 43.31 292.7L110.6 360C113.2 362.6 117.4 362.6 119.1 360C122.6 357.4 122.6 353.2 119.1 350.6L68.69 299.3zM520 350.6C517.4 353.2 517.4 357.4 520 360C522.6 362.6 526.8 362.6 529.4 360L596.7 292.7C602.9 286.4 613.1 286.4 619.3 292.7C625.6 298.9 625.6 309.1 619.3 315.3L552 382.6C549.4 385.2 549.4 389.4 552 392C554.6 394.6 558.8 394.6 561.4 392L612.7 340.7C618.9 334.4 629.1 334.4 635.3 340.7C641.6 346.9 641.6 357.1 635.3 363.3L584 414.6C581.4 417.2 581.4 421.4 584 424C586.6 426.6 590.8 426.6 593.4 424L612.7 404.7C618.9 398.4 629.1 398.4 635.3 404.7C641.6 410.9 641.6 421.1 635.3 427.3L578.5 484.1C560.7 501.1 536.4 512 511.2 512C458.6 512 416 469.4 416 416.8V330.4C416 319.4 424.1 310.4 436 310.4C447 310.4 456 319.4 456 330.4V350.1C456 357.2 464.6 360.8 469.7 355.7L548.7 276.7C554.9 270.4 565.1 270.4 571.3 276.7C577.6 282.9 577.6 293.1 571.3 299.3L520 350.6z"></path></svg></span>
                  <span>HuggingFace</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
        <img src="figures/teaser.png"  style="width:100%;">
      </center>
    </div>
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-four-fifths"> -->
        <div class="content has-text-justified">
          <p>
            When learning a new image manipulation operation that is <strong><i>unseen</i></strong> in the training set (as shown above), textual instructions directly point out the subject and provide high-level semantic guidance, while exemplar images mitigate linguistic ambiguity and show more local details that are difficult to describe in language. Our proposed multi-modal autoregressive model -- <strong><i>InstaManip</i></strong> takes advantage of both textual and visual guidance to learn a representation of the desired transformation, and applies it to a new query image.
          </p>
        </div>
      <!-- </div> -->
    </div>
  </div>
</section>
<!-- End teaser -->


<!-- Video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <br>
    <br>
    <div class="hero-body">
      <center>
        <iframe width="864" height="486" src="https://www.youtube.com/embed/JedJi_f-oQk?si=KQZ1flz85Hjq3SR2" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </center>
    </div>
  </div>
</section>
<!-- End video -->


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Abstract</h2>
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-four-fifths"> -->
        <div class="content has-text-justified">
          <p>
            Text-guided image manipulation has experienced notable advancement in recent years. 
            In order to mitigate linguistic ambiguity, few-shot learning with visual examples has been applied for instructions that are underrepresented in the training set, or difficult to describe purely in language. 
            However, learning from visual prompts requires strong reasoning capability, which diffusion models are struggling with. 
            To address this issue, we introduce a novel multi-modal autoregressive model, dubbed InstaManip, that can instantly learn a new image manipulation operation from textual and visual guidance via in-context learning, and apply it to new query images. 
            Specifically, we propose an innovative group self-attention mechanism to break down the in-context learning process into two separate stages -- learning and applying, which simplifies the complex problem into two easier tasks. 
            We also introduce a relation regularization method to further disentangle image transformation features from irrelevant contents in exemplar images. 
            Extensive experiments suggest that our method surpasses previous few-shot image manipulation models by a notable margin (≥ 19% in human evaluation). 
            We also find our model can be further boosted by increasing the number or diversity of exemplar images.
          </p>
        </div>
      <!-- </div> -->
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h2 class="title is-3">The Proposed Method</h2>
        <div class="container is-max-desktop">
          <div class="hero-body">
            <center>
              <img src="figures/method.png"  style="width:100%;">
            </center>
          </div>
        </div>
        
        <h2 class="title is-3">Visualization</h2>
        <div class="container is-max-desktop">
          <div class="hero-body">
            <center>
              <img src="figures/demo1.png"  style="width:100%;">
            </center>
          </div>
        </div>

        <h2 class="title is-3">Ablation Study</h2>
        <div class="container is-max-desktop">
          <div class="hero-body">
            <center>
              <img src="figures/ablation.png"  style="width:80%;">
            </center>
          </div>
        </div>

        <h2 class="title is-3">Same Textual Instructions + Different Exemplar Images</h2>
        <div class="container is-max-desktop">
          <div class="hero-body">
            <center>
              <img src="figures/same_txt_diff_img.png"  style="width:100%;">
            </center>
          </div>
        </div>

        <h2 class="title is-3">Scaling Up with More Exemplar Images</h2>
        <div class="container is-max-desktop">
          <div class="hero-body">
            <center>
              <img src="figures/scaleup.png"  style="width:80%;">
            </center>
          </div>
        </div>

        <h2 class="title is-3">Additional Visualization</h2>
        <div class="container is-max-desktop">
          <div class="hero-body">
            <center>
              <img src="figures/demo2.png"  style="width:100%;">
            </center>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{lai2024unleashing,
      title={Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation},
      author={Lai, Bolin and Juefei-Xu, Felix and Liu, Miao and Dai, Xiaoliang and Mehta, Nikhil and Zhu, Chenguang and Huang, Zeyi and Rehg, James M and Lee, Sangmin and Zhang, Ning and Xiao, Tong},
      journal={arXiv preprint arXiv:2412.01027},
      year={2024}}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            We appreciate the original source code from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
